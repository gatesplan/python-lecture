<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ML 5. 전처리와 KNN - 정리 노트</title>
    
    <!-- MathJax for mathematical formulas -->
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']]
            }
        };
    </script>
    
    <style>
        /* 기존 스타일 기반으로 수정 */
        @page {
            size: A4;
            margin: 2cm;
        }

        body {
            font-family: 'Malgun Gothic', sans-serif;
            line-height: 1.6;
            color: #000;
            background: #fff;
            margin: 0;
            padding: 20px;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0;
        }

        /* 제목 스타일 */
        h1 {
            font-size: 28px;
            font-weight: bold;
            text-align: center;
            margin: 0 0 40px 0;
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 15px;
        }

        h2 {
            font-size: 22px;
            font-weight: bold;
            margin: 30px 0 20px 0;
            color: #34495e;
            border-left: 5px solid #3498db;
            padding-left: 15px;
        }

        h3 {
            font-size: 18px;
            font-weight: bold;
            margin: 25px 0 15px 0;
            color: #2c3e50;
        }

        h4 {
            font-size: 16px;
            font-weight: bold;
            margin: 20px 0 10px 0;
            color: #34495e;
        }

        /* 섹션 스타일 */
        .section {
            margin-bottom: 40px;
            padding: 20px;
            background: #f8f9fa;
            border-radius: 8px;
            border: 1px solid #e9ecef;
        }

        .subsection {
            margin-bottom: 25px;
            padding: 15px;
            background: #ffffff;
            border-radius: 5px;
            border-left: 3px solid #17a2b8;
        }

        /* 코드 블록 스타일 */
        .code-block {
            background: #f4f4f4;
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 15px;
            margin: 15px 0;
            font-family: 'Consolas', 'Monaco', 'Courier New', monospace;
            font-size: 14px;
            line-height: 1.4;
            white-space: pre-wrap;
            overflow-x: auto;
        }

        .code-title {
            background: #34495e;
            color: white;
            padding: 8px 15px;
            margin: 15px 0 0 0;
            font-size: 13px;
            font-weight: bold;
            border-radius: 5px 5px 0 0;
        }

        .code-block-with-title {
            margin-top: 0;
            border-radius: 0 0 5px 5px;
        }

        /* 수식 스타일 */
        .formula {
            background: #fff3cd;
            border: 2px solid #ffeaa7;
            border-radius: 8px;
            padding: 20px;
            margin: 20px 0;
            text-align: center;
            font-size: 16px;
            font-weight: bold;
        }

        /* 설명 박스 */
        .explanation {
            background: #e8f5e8;
            border: 1px solid #c3e6c3;
            border-radius: 5px;
            padding: 15px;
            margin: 15px 0;
            font-style: italic;
        }

        .explanation::before {
            content: "💡 ";
            font-style: normal;
            font-weight: bold;
        }

        /* 키워드 박스 */
        .keywords {
            background: #f0f8ff;
            border: 2px solid #87ceeb;
            border-radius: 8px;
            padding: 20px;
            margin: 20px 0;
        }

        .keywords h3 {
            margin-top: 0;
            color: #1e90ff;
        }

        .keyword-list {
            list-style-type: none;
            padding: 0;
        }

        .keyword-list li {
            padding: 8px 0;
            border-bottom: 1px solid #e0e0e0;
        }

        .keyword-list li:last-child {
            border-bottom: none;
        }

        .keyword-code {
            font-family: 'Consolas', 'Monaco', 'Courier New', monospace;
            background: #f0f0f0;
            padding: 2px 6px;
            border-radius: 3px;
            font-size: 13px;
        }

        /* 비교 박스 스타일 */
        .comparison {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin: 20px 0;
        }

        .comparison-item {
            padding: 15px;
            border-radius: 8px;
            border: 2px solid;
        }

        .pca-item {
            background: #e3f2fd;
            border-color: #2196f3;
        }

        .tsne-item {
            background: #f3e5f5;
            border-color: #9c27b0;
        }

        /* 경고 박스 */
        .warning {
            background: #fff3cd;
            border: 1px solid #ffeaa7;
            border-radius: 5px;
            padding: 15px;
            margin: 15px 0;
        }

        .warning::before {
            content: "⚠️ ";
            font-weight: bold;
        }

        /* 프린트 최적화 */
        @media print {
            body {
                font-size: 12px;
            }
            
            /* 큰 섹션만 페이지 나눔 방지 */
            .section {
                page-break-inside: avoid;
            }
            
            /* 긴 코드 블록은 페이지 나눔 허용 */
            .code-block {
                page-break-inside: auto;
            }
            
            /* 제목 뒤의 페이지 나눔만 방지 */
            h1, h2 {
                page-break-after: avoid;
            }
            
            /* h3는 자연스럽게 페이지 나눔 허용 */
            h3 {
                page-break-after: auto;
            }
            
            /* 비교 박스는 함께 유지 */
            .comparison {
                page-break-inside: avoid;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>ML 5. 전처리와 KNN</h1>

        <div class="keywords">
            <h3>복습 키워드 (ML 4. 서포트 벡터 머신에서 배운 내용)</h3>
            <ul class="keyword-list">
                <li>1. 라벨 인코더 <span class="keyword-code">sklearn.preprocessing.LabelEncoder</span> → 문자열을 숫자로 변환</li>
                <li>2. 서포트 벡터 머신 <span class="keyword-code">sklearn.svm.SVC</span> → 결정 경계로 분류</li>
                <li>3. PCA 차원축소 <span class="keyword-code">sklearn.decomposition.PCA</span> → 고차원 데이터 시각화</li>
                <li>4. 분류 정확도 평가 <span class="keyword-code">accuracy_score, classification_report, confusion_matrix</span></li>
                <li>5. 핵심 개념: 결정 경계(decision boundary), 서포트 벡터(support vector), 마진(margin)</li>
            </ul>
        </div>

        <div class="section">
            <h2>1. 데이터 전처리 (Preprocessing)</h2>
            <div class="subsection">
                <div class="explanation">
                실제 데이터는 대부분 더럽다. 결측치가 있고, 이상치가 있고, 형식이 맞지 않는다. 이런 문제를 해결하는 과정이 전처리다.
                </div>
                
                <h3>1.1 데이터 탐색과 문제점 파악</h3>
                <div class="code-title">결측치 탐색하기</div>
                <div class="code-block code-block-with-title">import seaborn as sns

# 실제 더러운 데이터 로드
titanic = sns.load_dataset('titanic')

# 결측치가 있는 열만 추출
titanic.isna().sum()[titanic.isna().sum() > 0]
# 출력:
# age            177
# embarked         2
# deck           688
# embark_town      2</div>

                <h3>1.2 결측치 처리 전략</h3>
                <div class="code-title">결측치 처리 방법들</div>
                <div class="code-block code-block-with-title"># 전략 1: 평균값으로 채우기 (연속형 데이터)
mean_age = titanic['age'].mean()
titanic['age'] = titanic['age'].fillna(mean_age)

# 전략 2: 최빈값으로 채우기 (범주형 데이터)
freq = titanic['embark_town'].mode()[0]
titanic['embark_town'] = titanic['embark_town'].fillna(freq)

# 전략 3: 결측치가 너무 많은 열 삭제
titanic.drop(columns=['deck'], inplace=True)</div>

                <h3>1.3 범주형 데이터 숫자 변환</h3>
                <div class="explanation">
                머신러닝 모델은 숫자만 이해할 수 있다. 문자열 데이터를 숫자로 변환해야 한다.
                </div>
                
                <div class="code-title">LabelEncoder 일괄 처리</div>
                <div class="code-block code-block-with-title">from sklearn.preprocessing import LabelEncoder

# 범주형 데이터 라벨 인코딩
target_cols = ['sex', 'class', 'who', 'adult_male', 'embark_town', 'alive', 'alone']
encoders = {}

for col in target_cols:
    encoders[col] = LabelEncoder()
    clean_titanic[col] = encoders[col].fit_transform(clean_titanic[col])

# 전처리 완료된 데이터 저장
clean_titanic.to_csv('preprocessed_titanic.csv')</div>
            </div>
        </div>

        <div class="section">
            <h2>2. KNN 알고리즘</h2>
            <div class="subsection">
                <h3>2.1 KNN 기본 개념과 원리</h3>
                <div class="explanation">
                KNN은 "가장 가까운 이웃들의 투표로 분류하는 직관적인 방법"이다.
                </div>
                
                <h4>동작 원리:</h4>
                <ul>
                    <li>새로운 데이터가 들어오면, 가장 가까운 K개의 이웃을 찾는다</li>
                    <li>그 K개 이웃들이 어떤 클래스인지 확인한다</li>
                    <li>다수결로 새로운 데이터의 클래스를 결정한다</li>
                </ul>

                <h4>K값의 의미:</h4>
                <ul>
                    <li><strong>K=1:</strong> 가장 가까운 1개만 보고 결정 (과적합 위험)</li>
                    <li><strong>K=3:</strong> 가장 가까운 3개를 보고 다수결로 결정</li>
                    <li><strong>K=5:</strong> 가장 가까운 5개를 보고 다수결로 결정</li>
                    <li><strong>홀수 추천:</strong> 동점을 피하기 위해</li>
                </ul>

                <h3>2.2 KNN 모델 구현</h3>
                <div class="code-title">KNN 분류기 구현</div>
                <div class="code-block code-block-with-title">from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# 데이터 분할
X = clean_titanic.iloc[:, 1:]  # 특성
y = clean_titanic.iloc[:, 0]   # 타겟
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# KNN 모델 선언과 학습
model = KNeighborsClassifier(n_neighbors=5)
model.fit(X_train, y_train)

# 예측 및 평가
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"정확도: {accuracy:.3f}")</div>

                <h3>2.3 K값 최적화</h3>
                <div class="code-title">K값에 따른 성능 비교</div>
                <div class="code-block code-block-with-title"># 여러 K값으로 성능 비교
k_values = [1, 3, 5, 7, 15]
accuracies = []

for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train, y_train)
    y_pred_k = knn.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred_k)
    accuracies.append(accuracy)
    print(f"K={k}: 정확도 {accuracy:.3f}")

# 최고 성능 K값 찾기
best_k = k_values[accuracies.index(max(accuracies))]
print(f"🏆 최고 성능: K={best_k}")</div>
            </div>
        </div>

        <div class="section">
            <h2>3. 차원축소와 시각화: PCA vs t-SNE</h2>
            <div class="subsection">
                <div class="warning">
                <strong>중요:</strong> PCA와 t-SNE는 사용 목적과 방법이 완전히 다르다!
                </div>

                <div class="comparison">
                    <div class="comparison-item pca-item">
                        <h4>🔵 PCA - 시각화 목적의 사후 처리</h4>
                        <p><strong>목적:</strong> 이미 학습된 모델의 결과를 2D로 시각화</p>
                        <p><strong>사용법:</strong> 원본 고차원 데이터로 모델 학습 → PCA로 차원축소 → 시각화</p>
                    </div>
                    <div class="comparison-item tsne-item">
                        <h4>🟣 t-SNE - 공간 변환 후 학습</h4>
                        <p><strong>목적:</strong> 고차원 데이터를 저차원으로 변환한 새로운 공간에서 학습</p>
                        <p><strong>사용법:</strong> t-SNE로 차원축소 → 변환된 데이터로 모델 학습</p>
                    </div>
                </div>

                <h3>3.1 PCA 접근법 (시각화 목적)</h3>
                <div class="code-title">PCA 시각화 패턴</div>
                <div class="code-block code-block-with-title"># 1단계: 원본 고차원 데이터로 모델 학습
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)  # 12차원 원본 데이터 사용
y_pred = knn.predict(X_test)

# 2단계: 시각화 목적으로만 PCA 적용
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
X_test_pca = pca.transform(X_test)  # 2차원으로 축소

# 3단계: 시각화
plt.scatter(X_test_pca[:, 0], X_test_pca[:, 1], c=y_pred, cmap='viridis')
plt.title('PCA 변환 후 KNN 분류 결과 시각화')
plt.show()</div>

                <h3>3.2 t-SNE 접근법 (공간 변환 후 학습)</h3>
                <div class="code-title">t-SNE 학습 패턴</div>
                <div class="code-block code-block-with-title"># 1단계: t-SNE로 데이터 공간 변환
from sklearn.manifold import TSNE
tsne = TSNE(n_components=2, perplexity=10, max_iter=500)
X_tsne = tsne.fit_transform(X)  # 전체 데이터 변환

# 2단계: 변환된 데이터로 train/test 분할
X_train_tsne, X_test_tsne, y_train, y_test = train_test_split(X_tsne, y, test_size=0.2)

# 3단계: 변환된 2차원 데이터로 모델 학습
knn_tsne = KNeighborsClassifier(n_neighbors=5)
knn_tsne.fit(X_train_tsne, y_train)  # 2차원 변환 데이터 사용
y_pred_tsne = knn_tsne.predict(X_test_tsne)

# 4단계: 시각화
plt.scatter(X_test_tsne[:, 0], X_test_tsne[:, 1], c=y_pred_tsne, cmap='viridis')
plt.title('t-SNE 변환된 공간에서의 KNN 분류')
plt.show()</div>

                <h3>3.3 t-SNE 수학적 원리</h3>
                <div class="explanation">
                t-SNE는 고차원 공간의 거리 관계를 저차원 공간의 확률 분포로 변환한다.
                </div>

                <div class="formula">
                    <h4>1단계: 고차원 공간에서의 조건부 확률</h4>
                    $$p_{j|i} = \frac{\exp(-||x_i - x_j||^2 / 2\sigma_i^2)}{\sum_{k \neq i} \exp(-||x_i - x_k||^2 / 2\sigma_i^2)}$$
                </div>

                <div class="formula">
                    <h4>2단계: 저차원 공간에서의 확률</h4>
                    $$q_{ij} = \frac{(1 + ||y_i - y_j||^2)^{-1}}{\sum_{k \neq l}(1 + ||y_k - y_l||^2)^{-1}}$$
                </div>

                <div class="formula">
                    <h4>3단계: KL 발산 최소화</h4>
                    $$C = \sum_i KL(P_i||Q_i) = \sum_i \sum_j p_{j|i} \log \frac{p_{j|i}}{q_{j|i}}$$
                </div>

                <div class="explanation">
                t-SNE는 이 KL 발산을 최소화하여 고차원의 이웃 관계를 저차원에서 최대한 보존한다.
                </div>
            </div>
        </div>

        <div class="section">
            <h2>4. 모델 평가와 시각화</h2>
            <div class="subsection">
                <h3>4.1 성능 평가</h3>
                <div class="code-title">분류 성능 평가</div>
                <div class="code-block code-block-with-title">from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# 정확도
accuracy = accuracy_score(y_test, y_pred)
print(f"정확도: {accuracy:.3f}")

# 분류 리포트
print("분류 리포트:")
print(classification_report(y_test, y_pred))

# 혼동 행렬
print("혼동 행렬:")
print(confusion_matrix(y_test, y_pred))</div>

                <h3>4.2 시각화 비교</h3>
                <div class="code-title">PCA vs t-SNE 결과 비교</div>
                <div class="code-block code-block-with-title">import matplotlib.pyplot as plt
import koreanize_matplotlib

fig, axes = plt.subplots(1, 2, figsize=(15, 6))

# PCA 결과 시각화
axes[0].scatter(X_test_pca[:, 0], X_test_pca[:, 1], c=y_test, cmap='viridis', alpha=0.7)
axes[0].set_title('실제값 (PCA 변환 후 시각화)')

# t-SNE 결과 시각화  
axes[1].scatter(X_test_tsne[:, 0], X_test_tsne[:, 1], c=y_pred_tsne, cmap='viridis', alpha=0.7)
axes[1].set_title('t-SNE 변환 공간에서의 예측값')

plt.tight_layout()
plt.show()</div>
            </div>
        </div>

        <div class="section">
            <h2>5. 핵심 정리</h2>
            <div class="subsection">
                <h3>전처리의 중요성</h3>
                <ul>
                    <li>실제 데이터는 대부분 결측치와 이상치가 있다</li>
                    <li>결측치 처리: 평균값(연속형), 최빈값(범주형), 컬럼 삭제</li>
                    <li>LabelEncoder: 문자열 → 숫자 변환 필수</li>
                    <li>전처리 없이는 머신러닝 모델 학습 불가</li>
                </ul>

                <h3>KNN 알고리즘의 특징</h3>
                <ul>
                    <li><strong>장점:</strong> 직관적이고 이해하기 쉬움, 복잡한 경계도 학습 가능</li>
                    <li><strong>단점:</strong> 계산 비용이 높음, 차원의 저주에 민감</li>
                    <li><strong>K값 선택:</strong> 홀수 추천, 너무 작으면 과적합, 너무 크면 과소적합</li>
                    <li><strong>거리 기반:</strong> 특성 스케일링이 중요함</li>
                </ul>

                <h3>PCA vs t-SNE 선택 가이드</h3>
                <div class="comparison">
                    <div class="comparison-item pca-item">
                        <h4>PCA 사용 시점</h4>
                        <ul>
                            <li>고차원 모델의 결과를 시각화할 때</li>
                            <li>빠른 차원축소가 필요할 때</li>
                            <li>선형 관계를 보존하고 싶을 때</li>
                        </ul>
                    </div>
                    <div class="comparison-item tsne-item">
                        <h4>t-SNE 사용 시점</h4>
                        <ul>
                            <li>저차원 공간에서 모델을 학습하고 싶을 때</li>
                            <li>비선형 관계를 보존하고 싶을 때</li>
                            <li>클러스터 구조를 명확히 보고 싶을 때</li>
                        </ul>
                    </div>
                </div>

                <h3>주요 하이퍼파라미터</h3>
                <ul>
                    <li><strong>KNN:</strong> n_neighbors (이웃 수), weights (거리 가중치)</li>
                    <li><strong>PCA:</strong> n_components (주성분 개수)</li>
                    <li><strong>t-SNE:</strong> perplexity (이웃 수 관련), max_iter (반복 횟수)</li>
                </ul>
            </div>
        </div>
    </div>
</body>
</html>