{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 분류와 서포트 벡터 머신 (SVM)\n",
    "\n",
    "서포트 벡터 머신은 최적의 `결정 경계(decision boundary)`를 찾아 데이터를 분류하는 기계 학습 알고리즘이다.\n",
    "\n",
    "수학적 원리보다는 직관적 이해만 확보하고, 실제 구현에 집중한다.\n",
    "\n",
    "우리는 이번에 다음 두 가지 순서로 학습한다.\n",
    "\n",
    "1. 이론: 분류`Clustering`\n",
    "2. SVM 원리의 직관적 이해\n",
    "3. 구현과 실습"
   ],
   "id": "506ce7174b1c2900"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. 이론: 분류(Clustering)\n",
    "\n",
    "분류는 데이터를 정당한 기준을 정하여 n개의 그룹으로 나누는 작업이다.\n",
    "\n",
    "수학적인 방법으로 데이터를 분류하는 방법이 몇 가지가 있다.\n",
    "\n",
    "1. **SVM (Support Vector Machine)**: 데이터를 분류하는 결정 경계를 찾는 알고리즘\n",
    "2. **KNN (K-Nearest Neighbors)**: 새로운 데이터 포인트를 가장 가까운 K개의 이웃 데이터로 분류하는 알고리즘\n",
    "3. **Decision Tree**: 데이터를 분류하기 위해 트리 구조를 사용하는 알고리즘\n",
    "4. **Random Forest**: 여러 개의 결정 트리를 사용하여 데이터를 분류하는 앙상블 학습 방법\n",
    "\n",
    "이 외에도 많은 분류 알고리즘이 존재하지만, 우리는 `SVM`, `KNN`, `Random Forest`만 배우고 나머지는 소개만 한다.\n",
    "\n",
    "## 회귀 vs 분류\n",
    "\n",
    "회귀는 연속적인 값을 예측하는 작업이고, 분류는 데이터를 정해진 그룹으로 나누는 작업이다. 둘은 목적이 아예 다르다.\n"
   ],
   "id": "21d836c2b58ce95e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. SVM 원리의 직관적 이해\n",
    "\n",
    "구경 잘 하고, 용어 기억 할 것. 주요 용어는 다음과 같다.\n",
    "\n",
    "1. **결정 경계(decision boundary)**: 데이터를 분류하는 경계선\n",
    "2. **서포트 벡터(support vector)**: 결정 경계를 정의하는 데이터 포인트\n",
    "3. **마진(margin)**: 결정 경계와 서포트 벡터 사이의 거리"
   ],
   "id": "6247e0ffabb89c75"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# class A, class B 정의\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 중심이 (a, b), norm_dist d인 점 n개 생성\n",
    "def generate_points(a, b, d, n):\n",
    "    return np.random.normal(loc=[a, b], scale=d, size=(n, 2))\n",
    "\n",
    "cls_a = generate_points(0, 0, 1.5, 50)  # 클래스 A\n",
    "cls_b = generate_points(3, 3, 1.4, 50)  # 클래스 B\n",
    "\n",
    "# 클래스 A와 B의 점들을 시각화\n",
    "plt.scatter(cls_a[:, 0], cls_a[:, 1], label='Class A', color='blue')\n",
    "plt.scatter(cls_b[:, 0], cls_b[:, 1], label='Class B', color='red')\n",
    "plt.title('Class A and Class B Points')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "117bb0d2299fc83b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "위 두 클래스를 구분할 수 있는 적당한 선을 하나 상상해보자.\n",
    "\n",
    "SVM은 이 선을 찾는 수학적 방법이다.\n",
    "\n",
    "이때 둘을 구분하는 선을 **결정 경계(decision boundary)**라고 한다."
   ],
   "id": "730e46d5e283ef89"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 결정 경계 시각화\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(cls_a[:, 0], cls_a[:, 1], label='Class A', color='blue')\n",
    "plt.scatter(cls_b[:, 0], cls_b[:, 1], label='Class B', color='red')\n",
    "\n",
    "# SVM 모델 학습\n",
    "from sklearn import svm\n",
    "model = svm.SVC()  # 다항 커널 사용\n",
    "model.fit(np.vstack((cls_a, cls_b)), [0]*len(cls_a) + [1]*len(cls_b))\n",
    "\n",
    "# 결정 경계 그리기\n",
    "def plot_decision_boundary(model, X, ax=None):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "\n",
    "    xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 100),\n",
    "                         np.linspace(ylim[0], ylim[1], 100))\n",
    "    Z = model.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # 서포트 벡터 시각화\n",
    "    ax.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1],\n",
    "               s=100, facecolors='none', edgecolors='k', label='Support Vectors')\n",
    "\n",
    "    ax.contour(xx, yy, Z, colors='k', levels=[0], alpha=0.5,\n",
    "               linestyles='--')\n",
    "\n",
    "plot_decision_boundary(model, np.vstack((cls_a, cls_b)))\n",
    "plt.title('Decision Boundary for Class A and Class B')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "b66395e182f6ddb1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Practice1. 생각해보기\n",
    "\n",
    "1. SVM은 3개 이상의 클래스를 구분할 수 있을까?\n",
    "2. 만약 SVM으로 3개 이상의 클래스를 구분해야 한다면, 어떻게 하면 될까?\n",
    "3. SVM은 비선형 데이터도 분류할 수 있을까? 만약 그렇다면, 어떻게 하면 될까?\n",
    "4. SVM은 어떤 상황에서 잘 작동할까? 어떤 상황에서는 잘 작동하지 않을까?"
   ],
   "id": "f52ea6912b7ca8fb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. 구현과 실습\n",
    "\n",
    "SVM 모델을 실제로 사용할 수 있도록 단계별로 구현하고, 시각화하여 어떤 식으로 동작하는지 이해할 것이다.\n",
    "\n",
    "1. Encoding/Decoding\n",
    "2. SVM 모델 선언과 학습\n",
    "3. PCA 변환과 시각화"
   ],
   "id": "7c87c71e82db06c0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.1. Encoding/Decoding\n",
    "\n",
    "전처리 - 학습 - 평가 - 활용 과정 중 전처리 과정이다.\n",
    "\n",
    "`데이터 로드 -> 분류 라벨 판단 -> 숫자로 변환` 작업의 한 방법.\n",
    "\n",
    "`Label Encoding`은 문자열로 되어 있는 라벨 값을 숫자로 변환하는 방식이다. 구체적으로 두 가지를 배운다.\n",
    "\n",
    "1. `Label Encoding`: 문자열 라벨을 0 이상의 정수로 변환\n",
    "2. `Label Decoding`: 숫자 라벨을 문자열로 역변환\n",
    "3. `Label Classes`: 어떤 클래스가 있는지 조회\n",
    "4. 데이터셋에 인코딩한 라벨 추가하기"
   ],
   "id": "1dc2589141f3f345"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "iris = sns.load_dataset('iris')\n",
    "x = iris.iloc[:, :-1]\n",
    "y = iris.iloc[:, -1]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit_transform(y)"
   ],
   "id": "8e2ac5e163ea65bc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "le.fit(y)\n",
    "le.classes_\n",
    "le.transform(['setosa'])\n",
    "le.inverse_transform([1, 2])\n",
    "\n",
    "# 역변환\n",
    "yy_decoded = le.inverse_transform([0, 1, 2])\n",
    "yy_decoded"
   ],
   "id": "5cda43b9cce1a42",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 어떤 클래스가 있는지 조회\n",
    "le.classes_"
   ],
   "id": "53675393a6a6bdcb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "iris['species_encoded'] = y_encoded",
   "id": "b0ff2a6548f93f15",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "iris",
   "id": "8a3bf47b0e354f11",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Practice1. Label Encoding, Decoding\n",
    "\n",
    "1. `LabelEncoder`로 `titanic` 데이터셋의 `who`을 encoding/decoding하고 어떤 클래스가 있는지 확인해본다.\n",
    "2. `LabelEncoder`로 `titanic` 데이터셋의 `embark_town`을 encoding/decoding하고 어떤 클래스가 있는지 확인해본다.\n",
    "3. `LabelEncoder`로 `penguins` 데이터셋의 `species`을 encoding/decoding하고 어떤 클래스가 있는지 확인해본다.\n",
    "4. `LabelEncoder`로 `penguins` 데이터셋의 `island`을 encoding/decoding하고 어떤 클래스가 있는지 확인해본다."
   ],
   "id": "965c2aaecc6d11a1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(y)\n",
    "le.transform(y)\n",
    "le.classes_\n",
    "le.inverse_transform([1, 2])"
   ],
   "id": "e805867d5c182755",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1번\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "titanic = sns.load_dataset('titanic')\n",
    "le_who = LabelEncoder()\n",
    "le_who.fit(titanic['who'])\n",
    "le_who.classes_"
   ],
   "id": "3247d74d84910506",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2번\n",
    "col = 'embark_town'\n",
    "label_col = 'embark_town_label'\n",
    "\n",
    "le2 = LabelEncoder()\n",
    "titanic[label_col] = le2.fit_transform(titanic[col])\n",
    "print(\"클래스:\", le2.classes_)\n",
    "print(\"라벨:\", le2.transform(le2.classes_))\n",
    "\n",
    "titanic"
   ],
   "id": "586f80ae34a156",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3번\n",
    "le = LabelEncoder()\n",
    "penguins = sns.load_dataset('penguins')\n",
    "species_label = le.fit_transform(penguins['species'])\n",
    "penguins['species_label'] = species_label\n",
    "print(\"클래스:\", le.classes_)\n",
    "print(\"라벨:\", le.transform(le.classes_))\n",
    "penguins"
   ],
   "id": "bdea42e484bd1458",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4번\n",
    "island_label = le.fit_transform(penguins['island'])\n",
    "penguins['island_label'] = island_label\n",
    "print(\"클래스:\", le.classes_)\n",
    "print(\"라벨:\", le.transform(le.classes_))\n",
    "penguins"
   ],
   "id": "b7b53caffec334d4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 데이터프레임에 저장\n",
    "# titanic, who 열\n",
    "titanic = sns.load_dataset('titanic')\n",
    "t = titanic['who']\n",
    "le_who = LabelEncoder()\n",
    "titanic['who_label'] = le_who.fit_transform(t)\n",
    "titanic"
   ],
   "id": "e9120e236dbec7f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "penguins = sns.load_dataset('penguins')\n",
    "le_species = LabelEncoder()\n",
    "penguins['species_label'] = le_species.fit_transform(penguins['species'])\n",
    "penguins"
   ],
   "id": "9723903064ee768e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.2 SVM 모델 선언과 학습\n",
    "\n",
    "SVM 모델을 선언하고, 학습하는 과정을 살펴보자. 세부적으로 다음을 배운다.\n",
    "\n",
    "1. SVM 모델의 선언\n",
    "2. 모델 학습과 예측값 생성"
   ],
   "id": "71c7db2a0637bfd2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "iris = sns.load_dataset('iris')\n",
    "xx = iris.iloc[:, :-1]\n",
    "yy = iris.iloc[:, -1]\n",
    "\n",
    "x_train, x_test, y_train, y_test = tts(xx, yy, test_size=0.2)\n",
    "\n",
    "# 모델선언 kernel='linear' or kernel='rbf' or kernel='sigmoid'\n",
    "model = SVC(kernel='poly', degree=3)\n",
    "model.fit(x_train, y_train)"
   ],
   "id": "2aadc8484b3fbe25",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Practice2. 다른 커널을 사용한 모델 선언, 학습하기\n",
    "\n",
    "SVM은 다양한 커널 함수를 지원한다. `kernel` 파라미터를 변경하여 다른 커널을 사용해보자.\n",
    "\n",
    "지원하는 커널들 : `poly`, `rbf`, `sigmoid`\n",
    "\n",
    "<주의> `poly` 커널은 degree 파라메터를 필요로 한다. degree=4로 설정한다."
   ],
   "id": "ee44f87c45575bda"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1번\n",
    "lin_model_poly = svm.SVC(kernel='poly', degree=1)\n",
    "lin_model_poly.fit(x_train, y_train)"
   ],
   "id": "50e21d621947b95a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2번\n",
    "lin_model_rbf = svm.SVC()\n",
    "lin_model_rbf.fit(x_train, y_train)"
   ],
   "id": "a7fb1ad3e04be236",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3번\n",
    "lin_model_sigmoid = svm.SVC(kernel='sigmoid')\n",
    "lin_model_sigmoid.fit(x_train, y_train)"
   ],
   "id": "6c268acf9246a763",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "le_island = LabelEncoder()\n",
    "\n",
    "penguins.dropna(inplace=True)\n",
    "\n",
    "island_label = le_island.fit_transform(penguins['island'])\n",
    "penguins['island_label'] = island_label\n",
    "penguins.columns\n",
    "xx = penguins[['bill_length_mm', 'bill_depth_mm',\n",
    "       'flipper_length_mm', 'body_mass_g', 'island_label']]\n",
    "yy = penguins['species']\n",
    "\n",
    "x_train, x_test, y_train, y_test = tts(xx, yy, test_size=0.2)\n",
    "\n",
    "# SVM 모델 선언\n",
    "svc = SVC(kernel='rbf')\n",
    "svc.fit(x_train, y_train)\n",
    "\n",
    "y_pred = svc.predict(x_test)\n",
    "y_pred"
   ],
   "id": "e7f25e3661b03e3d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.3 PCA 변환과 시각화\n",
    "\n",
    "앞서 만든 SVC 모델의 분류 결과를 그래프로 그릴 것이다.\n",
    "\n",
    "이때 고차원 데이터는 평면에 나타내려면 차원 축소를 진행해야 한다.\n",
    "\n",
    "그 방법 중 하나로 주성분분석`PCA(Principal Component Analysis)`를 사용한다.\n",
    "\n",
    "이번에 할 내용은 아래와 같다.\n",
    "\n",
    "1. 고차원 데이터 선택\n",
    "2. PCA 변환기 선언\n",
    "3. PCA 변환 수행\n",
    "4. 시각화"
   ],
   "id": "d7ab508977429263"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 예시 생성을 위한 코드 - 배울 대상 아님\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import koreanize_matplotlib\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "\n",
    "# 2D 예시로 직관적 설명\n",
    "np.random.seed(42)\n",
    "# 기울어진 타원형 데이터 생성 - 유효한 공분산 행렬 사용\n",
    "mean = [0, 0]\n",
    "cov = [[3, 1.5], [1.5, 1]]  # 수정된 공분산 행렬\n",
    "data = np.random.multivariate_normal(mean, cov, 200)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# 왼쪽: 원본 데이터와 회전하는 축\n",
    "ax1.scatter(data[:, 0], data[:, 1], alpha=0.5)\n",
    "ax1.set_title('데이터를 여러 각도에서 보기')\n",
    "ax1.set_xlim(-6, 6)\n",
    "ax1.set_ylim(-6, 6)\n",
    "ax1.set_aspect('equal')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 오른쪽: 각 방향으로의 투영 분산\n",
    "angles = np.linspace(0, np.pi, 180)\n",
    "variances = []\n",
    "\n",
    "for angle in angles:\n",
    "    # 회전 벡터\n",
    "    direction = np.array([np.cos(angle), np.sin(angle)])\n",
    "    # 해당 방향으로 투영\n",
    "    projections = data @ direction\n",
    "    variances.append(np.var(projections))\n",
    "\n",
    "ax2.plot(angles * 180/np.pi, variances)\n",
    "ax2.set_xlabel('회전 각도 (도)')\n",
    "ax2.set_ylabel('투영 분산')\n",
    "ax2.set_title('각 방향으로 투영했을 때의 분산')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 최대 분산 방향 표시\n",
    "max_idx = np.argmax(variances)\n",
    "max_angle = angles[max_idx]\n",
    "ax2.axvline(max_angle * 180/np.pi, color='red', linestyle='--',\n",
    "            label=f'최대 분산 방향: {max_angle*180/np.pi:.1f}°')\n",
    "ax2.legend()\n",
    "\n",
    "# 원본 데이터에 주성분 축 그리기\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(data)\n",
    "\n",
    "# 주성분 벡터 그리기\n",
    "origin = np.mean(data, axis=0)\n",
    "for i, (comp, var) in enumerate(zip(pca.components_, pca.explained_variance_)):\n",
    "    ax1.arrow(origin[0], origin[1],\n",
    "              comp[0] * np.sqrt(var) * 2,\n",
    "              comp[1] * np.sqrt(var) * 2,\n",
    "              color=['red', 'blue'][i],\n",
    "              width=0.1,\n",
    "              head_width=0.3,\n",
    "              label=f'PC{i+1} ({pca.explained_variance_ratio_[i]:.1%})')\n",
    "ax1.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 공분산 행렬 검증\n",
    "print(\"공분산 행렬:\")\n",
    "print(cov)\n",
    "print(\"\\n고유값:\", np.linalg.eigvals(cov))"
   ],
   "id": "54762c8cd13cb74",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 필요한 모듈 임포트\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = sns.load_dataset('iris')\n",
    "xx = iris.iloc[:, :-1]\n",
    "yy = iris.iloc[:, -1]\n",
    "le = LabelEncoder()\n",
    "le.fit(yy)\n",
    "yy_encoded = le.transform(yy)\n",
    "iris['species_encoded'] = yy_encoded\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(xx, yy_encoded, test_size=0.2)\n",
    "\n",
    "# PCA 변환기 선언\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(xx)\n",
    "\n",
    "x_train_pca = pca.transform(x_train)\n",
    "x_test_pca = pca.transform(x_test)\n",
    "\n",
    "# SVC\n",
    "svc = SVC(kernel='rbf')\n",
    "svc.fit(x_train, y_train)\n",
    "y_pred = svc.predict(x_test)\n",
    "\n",
    "plt.scatter(x_train_pca[:, 0],\n",
    "            x_train_pca[:, 1],\n",
    "            c=y_train, cmap='viridis')\n",
    "plt.scatter(x_test_pca[:, 0],\n",
    "            x_test_pca[:, 1],\n",
    "            c=y_pred, cmap='viridis', marker='x', label='Test Data')"
   ],
   "id": "42fd23d888f69074",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###",
   "id": "6b24dcd332358b77"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 모델 평가\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# 세 가지 평가 방법\n",
    "# 1. 단순 분류 정확도\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ],
   "id": "dae71125605989b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. 분류 리포트\n",
    "report = classification_report(y_test, y_pred, target_names=le.classes_)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "# precision: 각 항목별 (테스트 데이터) 예측 성공률\n",
    "# recall: 각 항목별 (훈련 데이터) 예측 성공률\n",
    "# f1-score: precision과 recall의 조화 평균\n",
    "# support: 각 항목별 실제 데이터 개수\n",
    "# accuracy: 전체 데이터에 대한 정확도\n",
    "# macro avg: 각 항목별 f1-score의 단순평균 (항목 크기 차이 무시)\n",
    "# weighted avg: 각 항목별 f1-score의 가중평균 (항목 크기 반영)"
   ],
   "id": "6a1f1cd561e8125",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. 혼동 행렬\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# 시각화\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "plt.title('혼동행렬')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ],
   "id": "b265340ff6858fc4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Example. `titanic` 데이터셋에서 `embark_town` 분류하는 SVC 만들기\n",
    "\n",
    "앞서 배운 Label Encoding을 사용하여 문자로 된 열을 숫자 정보로 교체하고, 이후 SVC 모델로 원하는 분류 예측을 수행하는 예시이다.\n",
    "\n",
    "이 예시에서 Label Encoding을 언제, 왜 사용하는지 이해해야 한다."
   ],
   "id": "4a4f990cd86526d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "titanic = sns.load_dataset('titanic')\n",
    "titanic.head()"
   ],
   "id": "681e65672af739cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "yy = titanic['embark_town']\n",
    "le_y = LabelEncoder()\n",
    "titanic['embark_town_label'] = le_y.fit_transform(yy)\n",
    "titanic.head()"
   ],
   "id": "c71e8b3e112fc9b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# x 값으로 쓰고 싶은 열, survived, pclass, sex, age, fare, class, who\n",
    "# 문자 열들을 라벨로 변경\n",
    "le_sex = LabelEncoder()\n",
    "titanic['sex'] = le_sex.fit_transform(titanic.sex)\n",
    "le_who = LabelEncoder()\n"
   ],
   "id": "7b6b0df6de94bbba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Practice3. SVM 모델 선언, 학습, 평가하기\n",
    "\n",
    "1. `titanic` 데이터셋에서 생존 여부를 예측하는 SVM 모델을 만들고 평가하시오.\n",
    "\n",
    "2. `penguins` 데이터셋에서 종을 예측하는 SVM 모델을 만들고 평가하시오."
   ],
   "id": "8c46dd6194130352"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "titanic = sns.load_dataset('titanic')\n",
    "titanic"
   ],
   "id": "95970d0946a45e1d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "yy = titanic['survived']\n",
    "\n",
    "# sex, who, adult_male, embark_town, alone을 label 인코딩으로 변환\n",
    "le_sex = LabelEncoder()\n",
    "titanic['sex'] = le_sex.fit_transform(titanic.sex)\n",
    "le_who = LabelEncoder()\n",
    "titanic['who'] = le_who.fit_transform(titanic.who)\n",
    "le_adult_m = LabelEncoder()\n",
    "titanic['adult_male'] = le_adult_m.fit_transform(titanic.adult_male)\n",
    "le_embark_town = LabelEncoder()\n",
    "titanic['embark_town'] = le_embark_town.fit_transform(titanic.embark_town)\n",
    "le_alone = LabelEncoder()\n",
    "titanic['alone'] = le_alone.fit_transform(titanic.alone)\n",
    "\n",
    "titanic"
   ],
   "id": "c824c60c882de9f9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 정리\n",
   "id": "a22ee4b5d92163bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "titanic = sns.load_dataset('titanic')\n",
    "titanic\n",
    "\n",
    "le_alive = LabelEncoder()\n",
    "alive_label = le_alive.fit_transform(titanic['alive'])\n",
    "titanic['alive_label'] = alive_label\n",
    "\n",
    "xx = titanic[['fare', 'pclass', 'sibsp']]\n",
    "yy = titanic['alive_label']\n"
   ],
   "id": "cde9830fbc3ba9e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# tts\n",
    "x_train, x_test, y_train, y_test = train_test_split(xx, yy, test_size=0.2)"
   ],
   "id": "9b3fc813d8b9869c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# PCA 차원감소\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(x_train)\n",
    "x_train_pca = pca.transform(x_train)\n",
    "x_test_pca = pca.transform(x_test)"
   ],
   "id": "b84c61fd40317c6a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# SVC 모델\n",
    "svc = SVC(kernel='rbf')\n",
    "svc.fit(x_train, y_train)\n",
    "y_pred = svc.predict(x_test)\n",
    "y_pred"
   ],
   "id": "dfe39fa57a93025",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 시각화\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title('SVC Model on Titanic Dataset')\n",
    "\n",
    "# 점\n",
    "plt.scatter(x_train_pca[:, 0],\n",
    "            x_train_pca[:, 1],\n",
    "            c=y_train, cmap='jet',\n",
    "            label='Train', alpha=0.3)\n",
    "\n",
    "# 점\n",
    "plt.scatter(x_test_pca[:, 0],\n",
    "            x_test_pca[:, 1],\n",
    "            c=y_pred, cmap='jet',\n",
    "            label='Test', marker='d', alpha=0.7)\n",
    "\n",
    "plt.show()"
   ],
   "id": "6ad7e9ddc12912b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# SVC 점수\n",
    "svc.score(x_test, y_test)\n"
   ],
   "id": "795f12bf6d904437",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f1b1e6a399809ec5",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
