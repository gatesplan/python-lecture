<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ML 6. 의사결정트리, 랜덤포레스트와 앙상블 기법 - 정리 노트</title>
    <style>
        /* 기존 스타일 기반으로 수정 */
        @page {
            size: A4;
            margin: 2cm;
        }

        body {
            font-family: 'Malgun Gothic', sans-serif;
            line-height: 1.6;
            color: #000;
            background: #fff;
            margin: 0;
            padding: 20px;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0;
        }

        /* 제목 스타일 */
        h1 {
            font-size: 28px;
            font-weight: bold;
            text-align: center;
            margin: 0 0 40px 0;
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 15px;
        }

        h2 {
            font-size: 22px;
            font-weight: bold;
            margin: 30px 0 20px 0;
            color: #34495e;
            border-left: 5px solid #3498db;
            padding-left: 15px;
        }

        h3 {
            font-size: 18px;
            font-weight: bold;
            margin: 25px 0 15px 0;
            color: #2c3e50;
        }

        /* 섹션 스타일 */
        .section {
            margin-bottom: 40px;
            padding: 20px;
            background: #f8f9fa;
            border-radius: 8px;
            border: 1px solid #e9ecef;
        }

        .subsection {
            margin-bottom: 25px;
            padding: 15px;
            background: #ffffff;
            border-radius: 5px;
            border-left: 3px solid #17a2b8;
        }

        /* 코드 블록 스타일 */
        .code-block {
            background: #f4f4f4;
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 15px;
            margin: 15px 0;
            font-family: 'Consolas', 'Monaco', 'Courier New', monospace;
            font-size: 14px;
            line-height: 1.4;
            white-space: pre-wrap;
            overflow-x: auto;
        }

        .code-title {
            background: #34495e;
            color: white;
            padding: 8px 15px;
            margin: 15px 0 0 0;
            font-size: 13px;
            font-weight: bold;
            border-radius: 5px 5px 0 0;
        }

        .code-block-with-title {
            margin-top: 0;
            border-radius: 0 0 5px 5px;
        }

        /* 수식 스타일 */
        .formula {
            background: #fff3cd;
            border: 2px solid #ffeaa7;
            border-radius: 8px;
            padding: 20px;
            margin: 20px 0;
            text-align: center;
            font-size: 16px;
            font-weight: bold;
        }

        /* 설명 박스 */
        .explanation {
            background: #e8f5e8;
            border: 1px solid #c3e6c3;
            border-radius: 5px;
            padding: 15px;
            margin: 15px 0;
            font-style: italic;
        }

        .explanation::before {
            content: "💡 ";
            font-style: normal;
            font-weight: bold;
        }

        /* 키워드 박스 */
        .keywords {
            background: #f0f8ff;
            border: 2px solid #87ceeb;
            border-radius: 8px;
            padding: 20px;
            margin: 20px 0;
        }

        .keywords h3 {
            margin-top: 0;
            color: #1e90ff;
        }

        .keyword-list {
            list-style-type: none;
            padding: 0;
        }

        .keyword-list li {
            padding: 8px 0;
            border-bottom: 1px solid #e0e0e0;
        }

        .keyword-list li:last-child {
            border-bottom: none;
        }

        .keyword-code {
            font-family: 'Consolas', 'Monaco', 'Courier New', monospace;
            background: #f0f0f0;
            padding: 2px 6px;
            border-radius: 3px;
            font-size: 13px;
        }

        /* 프린트 최적화 */
        @media print {
            body {
                font-size: 12px;
            }
            
            .section {
                page-break-inside: avoid;
            }
            
            .code-block {
                page-break-inside: avoid;
            }
            
            h1, h2, h3 {
                page-break-after: avoid;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>ML 6. 의사결정트리, 랜덤포레스트와 앙상블 기법</h1>

        <div class="keywords">
            <h3>복습 키워드</h3>
            <ul class="keyword-list">
                <li>1. 라벨 인코더 <span class="keyword-code">sklearn.preprocessing.LabelEncoder</span></li>
                <li>2. 서포트 벡터 머신 <span class="keyword-code">sklearn.svm.SVC</span></li>
                <li>3. KNN 분류기 <span class="keyword-code">sklearn.neighbors.KNeighborsClassifier</span></li>
                <li>4. PCA 변환 <span class="keyword-code">sklearn.decomposition.PCA</span> → 분류 → 결과를 시각화</li>
                <li>5. tSNE 변환 <span class="keyword-code">sklearn.manifold.TSNE</span> → 변환 후 KNN 분류</li>
                <li>6. 분류 정확도 평가 <span class="keyword-code">sklearn.metrics.accuracy_score</span></li>
            </ul>
        </div>

        <div class="section">
            <h2>1. 라벨 인코더 (Label Encoder)</h2>
            <div class="subsection">
                <div class="explanation">
                문자열 카테고리 데이터를 숫자로 변환하여 머신러닝 모델이 처리할 수 있도록 만드는 전처리 도구
                </div>
                
                <div class="code-title">라벨 인코더 기본 사용법</div>
                <div class="code-block code-block-with-title">from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
le.fit_transform(['cat', 'dog', 'fish', 'cat', 'dog'])
# 출력: array([0, 1, 2, 0, 1])

# 변환 및 역변환
le.transform(['cat', 'cat', 'cat', 'dog', 'fish'])
le.inverse_transform([0, 1, 2, 0])
le.classes_  # 클래스 확인</div>
            </div>
        </div>

        <div class="section">
            <h2>2. 서포트 벡터 머신 (SVM)</h2>
            <div class="subsection">
                <div class="explanation">
                데이터를 고차원 공간으로 매핑하여 선형 분리 가능한 형태로 변환 후 분류하는 알고리즘
                </div>

                <div class="code-title">SVM 분류기 구현</div>
                <div class="code-block code-block-with-title">from sklearn.svm import SVC

xs = [[0, 0, 1], [1, 0, 1], [0, 1, 1], [1, 1, 1],
      [0, 0, 2], [1, 0, 2], [0, 1, 2], [1, 1, 2]]
ys = [0, 1, 0, 1, 1, 1, 0, 1]

clf = SVC(kernel='rbf')
clf.fit(xs, ys)

y_pred = clf.predict([[0, 0, 1], [1, 0, 2], [0, 1, 2], [1, 1, 2]])</div>
            </div>
        </div>

        <div class="section">
            <h2>3. 차원 축소와 시각화</h2>
            
            <div class="subsection">
                <h3>3.1 PCA 변환</h3>
                <div class="explanation">
                고차원 데이터를 저차원으로 축소하여 시각화를 가능하게 하는 주성분 분석 기법
                </div>

                <div class="code-title">PCA를 이용한 차원 축소 및 시각화</div>
                <div class="code-block code-block-with-title">from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import koreanize_matplotlib

pca = PCA(n_components=2)
pca.fit(xs)
pca_xs = pca.transform(xs)  # 2차원으로 축소된 도메인

plt.scatter(pca_xs[:, 0], pca_xs[:, 1], c=ys, cmap='jet')
plt.title('PCA 변환 후 시각화')
plt.show()</div>
            </div>

            <div class="subsection">
                <h3>3.2 t-SNE 변환과 KNN 분류</h3>
                <div class="explanation">
                t-SNE로 차원을 축소한 후 KNN 분류기를 적용하는 복합적인 머신러닝 파이프라인
                </div>

                <div class="code-title">t-SNE + KNN 분류 파이프라인</div>
                <div class="code-block code-block-with-title">import seaborn as sns
from sklearn.manifold import TSNE
from sklearn.neighbors import KNeighborsClassifier

titanic = sns.load_dataset('titanic')
xs = titanic[['age', 'fare', 'sibsp', 'parch']]
ys = titanic['survived']

# t-SNE 차원 축소
tsne = TSNE(n_components=2, perplexity=10, max_iter=500)
xs_tsne = tsne.fit_transform(xs.fillna(0))

# KNN 분류기 적용
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(xs_tsne, ys)
predictions = knn.predict(xs_tsne)

# 시각화
plt.figure(figsize=(10, 6))
plt.scatter(xs_tsne[:, 0], xs_tsne[:, 1], c=ys, cmap='viridis', alpha=0.7)
plt.colorbar()
plt.title('tSNE 변환 후 시각화')
plt.xlabel('tSNE Component 1')
plt.ylabel('tSNE Component 2')
plt.show()</div>
            </div>
        </div>

        <div class="section">
            <h2>4. 불순도 (Impurity) 개념</h2>
            <div class="subsection">
                <div class="explanation">
                의사결정트리에서 데이터 집합의 "혼잡도"를 측정하는 지표. 값이 낮을수록 더 순수한(같은 클래스가 많은) 집합을 의미
                </div>

                <h3>4.1 지니 불순도 (Gini Impurity)</h3>
                <div class="formula">
                    Gini(S) = 1 - ∑(i=1 to c) p_i²
                </div>
                <div class="explanation">
                이탈리아 수학자 지니의 이름을 따서 만든 기법. 두 번의 독립시행에서 서로 다른 클래스가 나올 확률을 측정
                </div>

                <h3>4.2 엔트로피 (Entropy)</h3>
                <div class="formula">
                    Entropy(S) = -∑(i=1 to c) p_i log₂ p_i
                </div>
                <div class="explanation">
                정보 이론가 클로드 섀넌이 만든 "놀라움"의 정량화 기법. 평균 정보량을 측정하며, 값이 높을수록 더 무질서한 상태
                </div>

                <h3>4.3 지니 vs 엔트로피 선택 기준</h3>
                <div class="explanation">
                실제 성능 차이는 미미함. 둘 다 실험해보고 성능이 좋은 것을 선택하면 됨. 연구 목적으로는 엔트로피가 해석력 면에서 선호됨
                </div>
            </div>
        </div>

        <div class="section">
            <h2>5. 의사결정트리 구현</h2>
            <div class="subsection">
                <h3>5.1 데이터 전처리</h3>
                <div class="code-title">타이타닉 데이터 클리닝</div>
                <div class="code-block code-block-with-title">import seaborn as sns
from sklearn.preprocessing import LabelEncoder

titanic = sns.load_dataset('titanic')

# 결측치 확인
titanic.isna().sum()[titanic.isna().sum() > 0]

# 불필요한 열 제거
titanic = titanic.drop(columns=['deck', 'embarked', 'alive'])

# 결측치 처리
titanic['embark_town'] = titanic['embark_town'].fillna(titanic['embark_town'].mode()[0])
titanic['age'] = titanic['age'].fillna(titanic['age'].mean())

clean_titanic = titanic.copy()</div>

                <div class="code-title">라벨 인코딩 일괄 처리</div>
                <div class="code-block code-block-with-title"># 범주형 데이터 라벨 인코딩
target_cols = ['sex', 'class', 'who', 'adult_male', 'embark_town', 'alone']
encoders = {}

for col in target_cols:
    encoders[col] = LabelEncoder()
    clean_titanic[col] = encoders[col].fit_transform(clean_titanic[col])</div>
            </div>

            <div class="subsection">
                <h3>5.2 의사결정트리 모델 생성 및 평가</h3>
                <div class="code-title">의사결정트리 분류기 구현</div>
                <div class="code-block code-block-with-title">from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

xs = clean_titanic.iloc[:, 1:]
ys = clean_titanic.iloc[:, 0]
train_x, test_x, train_y, test_y = train_test_split(xs, ys)

tree = DecisionTreeClassifier(
    criterion='gini',           # 불순도 측정 방법
    max_depth=5,               # 최대 깊이 제한
    min_samples_split=30,      # 분할을 위한 최소 샘플 수
    min_samples_leaf=10        # 리프 노드의 최소 샘플 수
)
tree.fit(train_x, train_y)

y_pred = tree.predict(test_x)
accuracy = accuracy_score(test_y, y_pred)
print(f'정확도: {accuracy:.3f}')</div>

                <div class="code-title">의사결정트리 시각화</div>
                <div class="code-block code-block-with-title">from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

plt.figure(figsize=(20, 8))
plot_tree(tree,
          feature_names=xs.columns,
          class_names=['Not Survived', 'Survived'],
          filled=True,
          impurity=True,
          rounded=True,
          fontsize=10)
plt.show()</div>
            </div>
        </div>

        <div class="section">
            <h2>6. 랜덤포레스트와 앙상블 기법</h2>
            <div class="subsection">
                <div class="explanation">
                여러 약-분류기(60% 정확도)를 모아 강-분류기(80% 정확도)를 만드는 기법
                </div>

                <h3>6.1 랜덤포레스트의 특징</h3>
                <ul>
                    <li><strong>부트스트랩 샘플링:</strong> 중복을 허용하여 랜덤 샘플링한 데이터셋으로 각 트리 학습</li>
                    <li><strong>랜덤 피처 선택:</strong> 모든 특성 중 일부만 랜덤하게 선택하여 분할 기준 사용</li>
                    <li><strong>다수결 투표:</strong> 각 트리의 예측 결과를 다수결 투표로 최종 결정</li>
                    <li><strong>과적합 방지:</strong> 여러 트리를 결합하여 일반화 능력 향상</li>
                    <li><strong>변수 중요도:</strong> 각 특성의 예측 기여도 평가 가능</li>
                </ul>

                <div class="code-title">랜덤포레스트 구현</div>
                <div class="code-block code-block-with-title">from sklearn.ensemble import RandomForestClassifier

# 랜덤포레스트 모델 생성 및 학습
rf_clf = RandomForestClassifier(
    n_estimators=100,    # 트리 개수
    max_depth=6,         # 최대 깊이
    min_samples_leaf=10, # 리프 노드 최소 샘플 수
    random_state=42
)
rf_clf.fit(xs_train, ys_train)

# 예측 및 평가
rf_y_pred = rf_clf.predict(xs_test)
rf_accuracy = accuracy_score(ys_test, rf_y_pred)
print(f'랜덤포레스트 정확도: {rf_accuracy:.2f}')</div>

                <div class="code-title">랜덤포레스트 시각화</div>
                <div class="code-block code-block-with-title"># 첫 번째 트리 시각화
plt.figure(figsize=(30, 8))
plot_tree(rf_clf.estimators_[0],
          feature_names=xs.columns,
          class_names=['Not Survived', 'Survived'],
          filled=True,
          impurity=True,
          rounded=True,
          fontsize=10)
plt.show()</div>

                <div class="code-title">특성 중요도 시각화</div>
                <div class="code-block code-block-with-title"># 특성 중요도 분석
feature_importance = clf.feature_importances_
feature_names = xs.columns

plt.figure(figsize=(10, 6))
plt.barh(feature_names, feature_importance)
plt.xlabel('Importance')
plt.title('Feature Importance in Random Forest')
plt.show()</div>
            </div>
        </div>

        <div class="section">
            <h2>7. 핵심 정리</h2>
            <div class="subsection">
                <h3>의사결정트리의 장점</h3>
                <ul>
                    <li>직관적인 해석이 가능한 화이트박스 모델</li>
                    <li>범주형/연속형 데이터 모두 처리 가능</li>
                    <li>특별한 데이터 전처리 불필요</li>
                    <li>시각화를 통한 의사결정 과정 추적 가능</li>
                </ul>

                <h3>랜덤포레스트의 장점</h3>
                <ul>
                    <li>단일 의사결정트리 대비 높은 정확도</li>
                    <li>과적합 위험 감소</li>
                    <li>특성 중요도 자동 계산</li>
                    <li>결측치에 상대적으로 강건함</li>
                </ul>

                <h3>주요 하이퍼파라미터</h3>
                <ul>
                    <li><strong>max_depth:</strong> 트리의 최대 깊이 (과적합 제어)</li>
                    <li><strong>min_samples_split:</strong> 분할을 위한 최소 샘플 수</li>
                    <li><strong>min_samples_leaf:</strong> 리프 노드의 최소 샘플 수</li>
                    <li><strong>n_estimators:</strong> 랜덤포레스트의 트리 개수</li>
                </ul>
            </div>
        </div>
    </div>
</body>
</html>